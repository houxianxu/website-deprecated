<blockquote>
  <p>In this post, I try to discuss how we could come up with the logistic and softmax regression for classification. I also implement the algorithms for image classification with <a href="http://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 dataset</a> by Python (numpy). <a href="/implementation/LogisticRegression.html">The first one</a>) is binary classification using logistic regression, <a href="/implementation/One-vs-All-LogisticRegression.html">the second one</a> is multi-classification using logistic regression with one-vs-all trick and <a href="/implementation/SoftmaxRegression.html">the last one</a>) is mutli-classification using softmax regression.</p>
</blockquote>

<!-- more -->

<h2 id="problem-setting">1. Problem setting</h2>
<p>Classification problem is to classify different objects into different categories. It is like regression problem, except that the predictor y just has a small number of discrete values. For simplicity, we just focus on <strong>binary classification</strong> that y can take two values 1 or 0 (indicating two classes).</p>

<h2 id="basic-idea">2. Basic idea</h2>
<p>We could plot the data on a 2-D plane and try to figure out whether there is any structure of the data (see following figure).</p>

<!-- ![Scatter Plot of Two variables](/images/logisticRegression/1.png "Figure 1") -->
<p><img src="/images/logisticRegression/1.png" width="80%" /></p>

<p>From the particular example above, it is not hard to figure out we could find a line to separate the two classes. Specifically we divide the 2-D plane into 2 parts according to a line, and then we can predict new sample by observing which part it belongs to. Mathematically if <script type="math/tex">z = w_0 + w_1x_1 + w_2x_2</script> &gt;= 0, then y = 1; if <script type="math/tex">z = w_0 + w_1x_1 + w_2x_2</script> &lt; 0, then y = 0. We can regard the linear function <script type="math/tex">w^Tx</script> as a mapping from raw sample data (<script type="math/tex">x_1, x_2</script>) to classes scores. Intuitively we wish that the “correct” class has a score that is higher than the scores of “incorrect” classes.</p>

<h2 id="how-to-find-the-best-line">3. How to find the best line</h2>
<p>The hypothesis is a linear model <script type="math/tex">w_0 + w_1x_1 + w_2x_2 = W^TX</script>, the threshold is z = 0. The score value of z depends on the distance between the point and the target line, and the absolute value of z could be very large or small. We could <strong>normalize the distances</strong> for convenience, however, we had better not use linear normalization such as x / (max(x) - min(x)) and x / (std(x)), because the distinction between the two classes is more obvious when the absolution value of z is larger. Sigmoid or logistic function is well-known to be used here, following is the function and plot of sigmoid function.</p>

<script type="math/tex; mode=display">g(z) = \frac{1}{1 + e^{-z}}</script>

<!-- ![Sigmoid function](/images/logisticRegression/2.png "Figure 2") -->
<p><img src="/images/logisticRegression/2.png" width="100%" /></p>

<p>The new model for classification is:</p>

<p><script type="math/tex">h(x) = \frac{1}{1 + e^{-w^Tx}}</script>
We can see from the figure above that when z 0, g(z) 0.5 and when the absolute vaule of v is very large the g(z) is more close to 1. By feeding the score to sigmoid function, not only the scores can be normalized from 0 to 1, which can make it much easier to find the loss function, but also the result can be interpreted from probabilistic aspect.</p>

<h2 id="figure-out-the-loss-function">4. Figure out the loss function</h2>
<p>we need to find a way to measure the agreement between the predicted scores and the ground truth value.</p>

<p><strong>Naive idea</strong></p>

<p>We could use least square loss after normalizing the training data, the result is as following:
<script type="math/tex">L_0 = \frac{1}{m} \sum_{i=1}^m(h(x^{(i)}) - y^{(i)})^2 = \frac{1}{m} \sum_{i=1}^m(\frac{1}{1 + e^{-w^Tx^{(i)}}} - y^{(i)})^2</script>, where <script type="math/tex">x^{(i)}</script> is a vector for all features <script type="math/tex">x_j^{(i)}</script> (j=0,1, … , n) for single sample i, and <script type="math/tex">y^{(i)}</script> is the target value for this example. However this loss function is not a convex function because of sigmoid function used here, which will make it very difficult to find the w to opimize the loss.</p>

<p><strong>Can we do better?</strong></p>

<p>Because of this is a binary classification problems, we can compute the loss for the two classes respectively. When target y = 1, the loss had better be very large when <script type="math/tex">h(x) = \frac{1}{1 + e^{-w^Tx}}</script> is close to zero, and the loss should be very small when h(x) is close to one; in the same way, when target y = 0, the loss had better be very small when h(x) is close to zero, and the loss should be very large when h(x) is close to one. In fact, we can find this kind of function:</p>

<script type="math/tex; mode=display">% <![CDATA[
L(h(x), y) =\begin{cases} -log(h(x)) & y = 1\\ -log(1 - h(x)) & y  = 0 \end{cases} =L(h(x), y) = -ylog(h(x)) - (1-y)log(1-h(x)) %]]></script>

<p>So the total loss: <script type="math/tex">L(w) = - \frac{1}{m} \sum_{i = 1}^m [y^{(i)}logh(x^{(i)}) + (1 - y^{(i)}) log(1-h(x^{(i)}))]</script></p>

<p><script type="math/tex">x^{(i)}</script> is a vector for all <script type="math/tex">x_j</script> (j=0,1, … , n), and <script type="math/tex">y^{(i)}</script> is the target value for this example.</p>

<script type="math/tex; mode=display">h(x) = \frac{1}{1 + e^{-w^Tx}}</script>

<p>The plots of loss function are shown below, and they meet the desirable properties discribed above.
<!-- ![Loss function](/images/logisticRegression/3.png "Figure 3") -->
<img src="/images/logisticRegression/3.png" width="100%" /></p>

<h2 id="find-the-best-w-to-minimize-the-loss">5 Find the best w to minimize the loss</h2>
<p>Like <a href="/linear-regression-post/">linear regression</a> we can use <strong>gradient descent algorithm</strong> to optimize w step by step.
Compute the gradient for just one sample:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
     \begin{split} 
     \frac{\partial}{\partial w_j} L(w) 
     &= -(y \frac{1}{g(w^Tx)} - (1-y)  \frac{1}{1 - g(w^Tx)})  \frac{\partial}{\partial w_j} g(w^Tx) \\
     &= -(y \frac{1}{g(w^Tx)} - (1-y)  \frac{1}{1 - g(w^Tx)})  g(w^Tx)(1 - g(w^Tx)) \frac{\partial}{\partial w_j} w^Tx \\
     &= -(y(1-g(w^Tx)) - (1-y)g(w^Tx))x_j \\
     &= (h(x)-y)x_j                                    
    \end{split}
    \end{equation} %]]></script>

<p>So the gradients are as following when considering all the samples:</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial w_j} L(w) = \frac{1}{m} \sum_{i = 1}^m (h(x)-y)x_j</script>

<p>Then we can use <strong>batch decent algorithm</strong> or <strong>stochastic decent algorithm</strong> to optimize <strong>w</strong>, i.e, <script type="math/tex">w := w + \alpha \frac{\partial}{\partial w_j} L(w)</script></p>

<p>We can see that the gradient or partial derivative is the same as gradient of linear regression except for the h(x). We can get a better understanding of this when interpreting the loss function from probabilistic aspect.</p>

<h2 id="probabilistic-interpretation">6. Probabilistic interpretation</h2>
<p>Let us regard the value of h(x) as the probability:</p>

<script type="math/tex; mode=display">\begin{cases} P(y=1|x;w) = h(x) \\ P(y = 0 | x; w) = 1 - h(x) \end{cases} =P(y|x;w) = (h(x))^y(1-h(x))^{1-y}</script>

<p>So the likelihood is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
     \begin{split} 
     L(w) &= p(y|X; w) \\ 
     &= \prod_{i = 1}^m p(y^{(i)}|x^{(i)};w) \\
     &= \prod_{i = 1}^m (h(x^{(i)}))^{y^{(i)}} (1-h(x^{(i)}))^{1-y^{(i)}} \\                               
    \end{split}
    \end{equation} %]]></script>

<p>And the log likelihood:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
     \begin{split} 
     l(w) = log(L(w))
     &= \sum_{i = 1}^m y^{(i)} log(x^{(i)}) + (1 - y^{(i)}) log(1 - h(x^{(i)}))                            
    \end{split}
    \end{equation} %]]></script>

<p>This equation is the same as the the loss function when picking minus, so minimize the loss can be interpreted as maximize the likelihood of the y when given x <code class="highlighter-rouge">p(y|x)</code>. What’s more, the value of h(x) can be interpreted as the probability of the sample to be classified to y = 1. I think this is why most people prefer sigmoid function for normalization, theoretically we can choose other functions that smoothly increase from 0 to 1.</p>

<p>After we optimize the w, we get a line in 2-D space and the line is usually called decision boundary (h(x) = 0.5). We can also generalize to binary classification on n-D space, and the corresponding decision boundary is a (n-1) Dimension hyperplane (subspace) in n-D space.</p>

<p><a name="multiclass"></a></p>

<h2 id="multiclass-classification----one-vs-all">7. Multiclass classification – One vs all</h2>
<p>We need to generalize to the multiple class case, that’s to say, the value of y is not binary any more, instead y can equal to 0, 1, 2, …, k.</p>

<h4 id="basic-idea----transfer-multi-class-classification-into-binary-classification-problem">Basic idea – Transfer multi-class classification into binary classification problem</h4>
<p>We need change multiple classes into two classes, and the idea is to construct several logistic classifier for each class. We set the value of y (label) of one class to 1, and 0 for other classes. Thus, if we have K classes, we build K logistic classifiers and use it for prediction. There is a potential problem that one sample might be classified to several classes or non-class. The solution is to compare all the values of h(x) and classify the sample to the class with the highest value of h(x). The idea is shown in following figure (From Andrew’s notes).</p>

<!-- ![One vs all](/images/logisticRegression/4.png "Figure 4") -->

<p><img src="/images/logisticRegression/4.png" width="100%" /></p>

<h2 id="can-we-do-better----softmax">8. Can we do better? – Softmax</h2>
<p>In logistic regression classifier, we use linear function to map raw data (a sample) into a score z, which is feeded into logistic function for normalization, and then we interprete the results from logistic function as the probability of the “correct” class (y = 1). We just need a mapping function here because of just two classes (just need to decide whether one sample belongs to one class or not).
For multiple classes problems (K categories), it is possible to establish a mapping function for each class. As above we can simply use a linear mapping for all classes (K mapping function):</p>

<script type="math/tex; mode=display">f(x^{(i)}, W, b) = Wx{(i)} + b</script>

<p>Where <script type="math/tex">x^{(i)}</script> is a vector for all features <script type="math/tex">x_j^{(i)}</script> (j=0,1, … , n) for single sample i, and <script type="math/tex">x^{(i)}</script> is a single column vector of shape <script type="math/tex">[D, 1]</script>. <strong>W</strong> is a matrix of shape <script type="math/tex">[K, D]</script> called <strong>weights</strong>, <strong>K</strong> is the number of categories, and <strong>b</strong> is a vector of <script type="math/tex">[K, 1]</script> called <strong>bias vector</strong>. It is a little cumbersome to keep track of two sets of parameters (<strong>W</strong> and <strong>b</strong>), in factor we can combine the two into a single matrix. Specifically we can extend the feature vector <script type="math/tex">x^{(i)}</script> with an addition bias dimension holding constant 1, while extending <strong>W</strong> matrix with a new column (at the first or last column). Thus we get score mapping function:</p>

<script type="math/tex; mode=display">f(x^{(i)}, W) = Wx^{(i)}</script>

<p>Where <strong>W</strong> is a matrix of shape <script type="math/tex">[K, D+1]</script>, <script type="math/tex">x^{(i)}</script> is vector of shape <script type="math/tex">[D+1, 1]</script>, and <script type="math/tex">f(x^{(i)}, W)</script> is a vector of shape <script type="math/tex">[K, 1]</script> indicating the different scores of every class for the <script type="math/tex">i^{th}</script> sample.</p>

<h4 id="find-the-loss-function">Find the loss function</h4>
<p>Similar to logistic regression classifier, we need to normalize the scores from 0 to 1. However we should not use a linear normalization as discussed in the logistic regression because the bigger the score of one class is, the more chance the sample belongs to this category. What’s more, the chance is similar high when the scores are very large (see the plot of logistic function above).
Similar to logistic function, people use exponential function (non-linear) to preprocess the scores and then compute the percentage of each score in the sum of all the scores. What’s more, the percentages can be interpreted as the probability of each class for one sample. Here is formula for the <script type="math/tex">i^{th}</script> sample:</p>

<script type="math/tex; mode=display">h(x^{(i)}) = \frac{e^{w_{y_j}^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}}</script>

<p>Here is the plot of h(x) for two classes in 3D space, you can rotate the graph by clicking the arrows to get a better understanding the shape of the h(x).
Where <script type="math/tex">x^{(i)}</script> is vector of all features of sample i, <script type="math/tex">w_j</script> is the weights for the <script type="math/tex">j^{th}</script> class, and <script type="math/tex">y_j</script> is the correct class for the <script type="math/tex">i^{th}</script> sample.</p>

<object classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://fpdownload.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=7,0,0,0" width="600" height="700" id="function_plotter" align="middle">
  <param name="movie" value="http://dwudljvm154gg.cloudfront.net/graph3d.swf?lpf=e^x / (e^x%2Be^y)&amp;lpxmin=-5&amp;lpxmax=5&amp;lpymin=-5&amp;lpymax=5&amp;lpzmin=0&amp;lpzmax=1" />
  <param name="quality" value="high" />
  <param name="bgcolor" value="#ffffff" />
  <embed src="http://dwudljvm154gg.cloudfront.net/graph3d.swf?lpf=e^x / (e^x%2Be^y)&amp;lpxmin=-3&amp;lpxmax=3&amp;lpymin=-3&amp;lpymax=3&amp;lpzmin=0&amp;lpzmax=1" quality="high" bgcolor="#ffffff" width="700" height="750" name="function_plotter" align="middle" allowscriptaccess="sameDomain" type="application/x-shockwave-flash" pluginspage="http://www.macromedia.com/go/getflashplayer" />
</object>

<p>So why exponential function? In my opinion, it is natually to come up with.</p>

<ul>
  <li>It is a very simple and widely used non-linear function</li>
  <li>This function is strictly increasing</li>
  <li>This function is a convex function and its derivative is strictly increasing. That’s to say, when the score is large, then make it even more larger.</li>
</ul>

<p>The lesson is that we should put exponential function in our toolbox for non-linear problems</p>

<p>After normalizing the scores, we can use the same concept to define the loss function, which should make the loss small when the normalized score of h(x) is large, and penlize more when h(x) is small. Thus, we can use <script type="math/tex">-log(h(x))</script> to compute the loss, and the loss for one sample is as following:</p>

<script type="math/tex; mode=display">L_i = -log \big(h(x^{(i)})\big) = -log \big(\frac{e^{f_{y_j}^{(i)}}} {\sum_{j = 1}^k e^{f_j^{(i)}}}\big) = -log \big(\frac{e^{w_{y_j}^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}}\big)</script>

<p>Total loss for all sample is:</p>

<script type="math/tex; mode=display">L = \frac{1}{m} \sum_{i = 1}^m L_i = - \frac{1}{m} \sum_{i = 1}^m log \big(h(x^{(i)})\big) = - \frac{1}{m} \sum_{i = 1}^m log \big(\frac{e^{f_{y_j}^{(i)}}} {\sum_{j = 1}^k e^{f_j^{(i)}}}\big) = - \frac{1}{m} \sum_{i = 1}^m log \big(\frac{e^{w_{y_j}^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}}\big)</script>

<h4 id="calculate-the-gradient-one-sample">Calculate the gradient (one sample)</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
  \begin{split} 
      \nabla_{w_j} L_i &= - \nabla_{w_j} log \big(\frac{e^{w_{y_j}^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}}\big) \\
      &= -\nabla_{w_j} \big(w_{y_j}^Tx^{(i)}\big) + \nabla_{w_j} log \big(\sum_{j = 1}^k e^{w_j^Tx^{(i)}}\big)  \end{split}
  \end{equation} %]]></script>

<p>So the gradient with respect to <script type="math/tex">w_{y_j}</script> (<script type="math/tex">y_j</script> is the correct class):
<script type="math/tex">\nabla_{w_{y_j}} = -x^{(i)} + \frac{e^{w_j^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}} x^{(i)}</script></p>

<p>The gradient with respect to <script type="math/tex">w_j</script>:
<script type="math/tex">\nabla_{w_j} = \frac{e^{w_j^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}} x^{(i)}</script></p>

<h4 id="is-there-any-problem-with-the-loss-function">Is there any problem with the loss function</h4>
<p>When writing code to implement the softmax function in practice, we should first compute the intermediate terms <script type="math/tex">e^{f_j}</script> to make the scores bigger and use a logarithm function to make the score smaller. However, the value of <script type="math/tex">e^{f_j}</script> may be very large due to the exponentials and dividing large numbers could be numerically unstable, so we should make <script type="math/tex">e^{f_j}</script> smaller before division. Here is the trick by multiply the numerator and denominator by a constant C:</p>

<script type="math/tex; mode=display">\frac{e^{f_{y_j}^{(i)}}} {\sum_{j = 1}^k e^{f_j^{(i)}}} = \frac{C e^{f_{y_j}^{(i)}}} {C \sum_{j = 1}^k e^{f_j^{(i)}}} = \frac{e^{f_{y_j}^{(i)} + logC}} {\sum_{j = 1}^k e^{f_j^{(i)} + logC}}</script>

<p>Because we have the flexibility to choose any number of C, we can choose C to make <script type="math/tex">e^{f_j^{(i)}} + logC</script> small. A common choice for C is to set <script type="math/tex">logC = -max_jf_j^{(i)}</script>. This trick makes the highest value of <script type="math/tex">f_j^{(i)} + logC</script> to be zero and less than 0 for others. So the values of <script type="math/tex">e^{f_j^{(i)}} + logC</script> are restricted from 0 to 1, which should be more appropriate for division.</p>

<h4 id="probabilistic-interpretation-1">Probabilistic interpretation</h4>
<p>We can interpret <script type="math/tex">h(x) = P(y^{(i)}) = \frac{e^{w_{y_j}^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}}</script> as the normalized probability of assigned to the correct label <script type="math/tex">y^{(i)}</script> given sample x^{(i)} and parameters <strong>W</strong>. Firstly the score <script type="math/tex">f(x^{(i)}, W) = Wx^{(i)}</script> can be interpreted as the unnormalized log probabilities. Then exponentiating the scores with on-linear function <script type="math/tex">e^x</script> gives the unnormalized probabilities (may call frequency). Last using division for normalization to make the probabilities sum to one. Like logistic regression, the minimize the negative log likelihood of the correct class can also be interpreted as performing <strong>Maximum Likelihood Estimation</strong>. The loss function can be also deduced from probabilistic theory like logistic regression, in fact linear regression, logistic regression and softmax regression all belong to <a href="http://en.wikipedia.org/wiki/Generalized_linear_model">Generalized Linear Model</a>.</p>

<h2 id="regularization-to-avoid-overfitting">8. Regularization to avoid overfitting</h2>
<p>In practice we often add a <strong>regularization loss</strong> to the loss function provided above to penalize large <strong>weights</strong> to improve generalization. The most common regularization penalty <strong>R(W)</strong> is the <strong>L2</strong> norm.</p>

<script type="math/tex; mode=display">R(W) =  \sum_k  \sum_d W_{k, d}^2</script>

<p>So the total loss is the <strong>data loss</strong> and the <strong>regularization loss</strong>, so the full loss becomes:</p>

<script type="math/tex; mode=display">L = \frac{1}{m} \sum_{i = 1}^m L_i + \frac{1}{2} \lambda \sum_k  \sum_d W_{k, d}^2</script>

<p>The advantage of penalizing large weights is to improve generalization and make the trained model work well for unseen data, because it means that no input dimension can have a very large influence on the scores all by itself and the final classifier is encouraged to take into account allnput dimensions to small amounts rather than a few dimensions and very strongly. Note that biases do not have the same effect as other parameters and do not control the strength of influence of an input dimension. So some people only regularize the weights <strong>W</strong> but not the biases, however, I regularize both in the implementation both for simplicity and better performance.</p>

<p>I have written <strong>another post</strong> to discuss regularization in more details, especially how to interpret it. You can find the post <a href="">here</a>.</p>

<h2 id="get-your-hands-dirty-and-have-fun">9. Get your hands dirty and have fun</h2>
<ul>
  <li>Purpose: Implement logistic regression and softmax regression classifier.</li>
  <li>Data: CIFAR-10 dataset, consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The data is available <a href="http://www.cs.toronto.edu/~kriz/cifar.html">here</a>.</li>
  <li>Setup: I choose Python (IPython, numpy etc.) on Mac for implementation, and the results are published in a IPython notebook.
    <ul>
      <li><a href="/implementation/LogisticRegression.html">click here</a> for logistic regression classification.</li>
      <li><a href="/implementation/One-vs-All-LogisticRegression.html">click here</a> for logistic multi-classification by one-vs-all trick.</li>
      <li><a href="/implementation/SoftmaxRegression.html">click here</a> for softmax multi-classification.</li>
    </ul>
  </li>
  <li>Following is code to implement the logistic, one-vs-all and softmax classifiers by gradient decent algorithm.</li>
</ul>

<p><strong>classifiers: algorithms/classifiers.py</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># file: algorithms/classifiers.py</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">algorithms.classifiers.loss_grad_logistic</span> <span class="kn">import</span> <span class="o">*</span> 

<span class="k">class</span> <span class="nc">LinearClassifier</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">None</span> <span class="c"># set up the weight matrix </span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
              <span class="n">reg</span> <span class="o">=</span> <span class="mf">1e3</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">vectorized</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="s">"""
        Train linear classifier using batch gradient descent or stochastic gradient descent

        Parameters
        ----------
        X: (D x N) array of training data, each column is a training sample with D-dimension.
        y: (N, ) 1-dimension array of target data with length N.
        method: (string) determine whether using 'bgd' or 'sgd'.
        batch_size: (integer) number of training examples to use at each step.
        learning_rate: (float) learning rate for optimization.
        reg: (float) regularization strength for optimization.
        num_iters: (integer) number of steps to take when optimization.
        verbose: (boolean) if True, print out the progress (loss) when optimization.

        Returns
        -------
        losses_history: (list) of losses at each training iteration
        """</span>

        <span class="n">dim</span><span class="p">,</span> <span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">num_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="c"># assume y takes values 0...K-1 where K is number of classes</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c"># initialize the weights with small values</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span> <span class="o">==</span> <span class="s">'Logistic'</span><span class="p">:</span> <span class="c"># just need weights for one class</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span>
            <span class="k">else</span><span class="p">:</span> <span class="c"># weigths for each class</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span>

        <span class="n">losses_history</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s">'bgd'</span><span class="p">:</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_grad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">vectorized</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c"># randomly choose a min-batch of samples</span>
                <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">num_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_grad</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">idxs</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">idxs</span><span class="p">],</span> <span class="n">reg</span><span class="p">,</span> <span class="n">vectorized</span><span class="p">)</span> <span class="c"># grad =[K x D]</span>
            <span class="n">losses_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="c"># update weights</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span> <span class="c"># [K x D]</span>
            <span class="c"># print self.W</span>
            <span class="c"># print 'dsfad', grad.shape</span>
            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">print</span> <span class="s">'iteration </span><span class="si">%</span><span class="s">d/</span><span class="si">%</span><span class="s">d: loss </span><span class="si">%</span><span class="s">f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">losses_history</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="s">"""
        Predict value of y using trained weights

        Parameters
        ----------
        X: (D x N) array of data, each column is a sample with D-dimension.

        Returns
        -------
        pred_ys: (N, ) 1-dimension array of y for N sampels
        """</span>
        <span class="n">pred_ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span> <span class="o">==</span> <span class="s">'Logistic'</span><span class="p">:</span>
            <span class="n">pred_ys</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">=</span><span class="mi">0</span> 
        <span class="k">else</span><span class="p">:</span> <span class="c"># multiclassification</span>
            <span class="n">pred_ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred_ys</span>

    <span class="k">def</span> <span class="nf">loss_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">vectorized</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="s">"""
        Compute the loss and gradients.

        Parameters
        ----------
        The same as self.train()

        Returns
        -------
        a tuple of two items (loss, grad)
        loss: (float)
        grad: (array) with respect to self.W
        """</span>
        <span class="k">pass</span>

<span class="c">### Subclasses of linear classifier</span>
<span class="k">class</span> <span class="nc">Logistic</span><span class="p">(</span><span class="n">LinearClassifier</span><span class="p">):</span>
    <span class="s">"""A subclass for binary classification using logistic function"""</span>
    <span class="k">def</span> <span class="nf">loss_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">vectorized</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">vectorized</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss_grad_logistic_vectorized</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss_grad_logistic_naive</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">LinearClassifier</span><span class="p">):</span>
    <span class="s">"""A subclass for multi-classicication using Softmax function"""</span>
    <span class="k">def</span> <span class="nf">loss_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">vectorized</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">vectorized</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss_grad_softmax_vectorized</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss_grad_softmax_naive</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span></code></pre></figure>

<p><strong>Function to compute loss and gradients for logistic classification: algorithms/classifiers/loss_grad_logistic.py</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># file: algorithms/classifiers/loss_grad_logistic.py</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">loss_grad_logistic_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
    <span class="s">"""
    Compute the loss and gradients using logistic function 
    with loop, which is slow.

    Parameters
    ----------
    W: (1, D) array of weights, D is the dimension of one sample.
    X: (D x N) array of training data, each column is a training sample with D-dimension.
    y: (N, ) 1-dimension array of target data with length N.
    reg: (float) regularization strength for optimization.

    Returns
    -------
    a tuple of two items (loss, grad)
    loss: (float)
    grad: (array) with respect to self.W
    """</span>
    <span class="n">dim</span><span class="p">,</span> <span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="c"># [1, D]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_train</span><span class="p">):</span>
        <span class="n">sample_x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">f_x</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">sample_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">f_x</span> <span class="o">+=</span> <span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">sample_x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">h_x</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">f_x</span><span class="p">))</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">h_x</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h_x</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">loss</span>
        <span class="n">grad</span> <span class="o">+=</span> <span class="p">(</span><span class="n">h_x</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">sample_x</span> <span class="c"># [D, ]</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="n">num_train</span>
    <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span> <span class="c"># add regularization</span>

    <span class="n">grad</span> <span class="o">/=</span> <span class="n">num_train</span>
    <span class="n">grad</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span> <span class="c"># add regularization</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span>

<span class="k">def</span> <span class="nf">loss_grad_logistic_vectorized</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
    <span class="s">"""Compute the loss and gradients with weights, vectorized version"""</span>
    <span class="n">dim</span><span class="p">,</span> <span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="c"># [1, D]</span>
    <span class="c"># print W</span>
    <span class="n">f_x_mat</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c"># [1, D] * [D, N]</span>
    <span class="n">h_x_mat</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">f_x_mat</span><span class="p">))</span> <span class="c"># [1, N]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">h_x_mat</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h_x_mat</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">num_train</span> <span class="o">*</span> <span class="n">loss</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_x_mat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c"># [1, D]</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">num_train</span> <span class="o">*</span> <span class="n">grad</span> <span class="o">+</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
    
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span></code></pre></figure>

<p><strong>Function to compute loss and gradients for softmax classification: algorithms/classifiers/loss_grad_softmax.py</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># file: algorithms/classifiers/loss_grad_softmax.py</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">loss_grad_softmax_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
    <span class="s">"""
    Compute the loss and gradients using softmax function 
    with loop, which is slow.

    Parameters
    ----------
    W: (K, D) array of weights, K is the number of classes and D is the dimension of one sample.
    X: (D, N) array of training data, each column is a training sample with D-dimension.
    y: (N, ) 1-dimension array of target data with length N with lables 0,1, ... K-1, for K classes
    reg: (float) regularization strength for optimization.

    Returns
    -------
    a tuple of two items (loss, grad)
    loss: (float)
    grad: (K, D) with respect to W
    """</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
    <span class="n">dim</span><span class="p">,</span> <span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_train</span><span class="p">):</span>
        <span class="n">sample_x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span> <span class="c"># [K, 1] unnormalized score</span>
        <span class="k">for</span> <span class="n">cls</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">cls</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">scores</span><span class="p">[</span><span class="n">cls</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">sample_x</span><span class="p">)</span>
        <span class="c"># Shift the scores so that the highest value is 0</span>
        <span class="n">scores</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">correct_class</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">sum_exp_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>

        <span class="n">corr_cls_exp_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="n">correct_class</span><span class="p">])</span>
        <span class="n">loss_x</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">corr_cls_exp_score</span> <span class="o">/</span> <span class="n">sum_exp_scores</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_x</span>

        <span class="c"># compute the gradient</span>
        <span class="n">percent_exp_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span> <span class="o">/</span> <span class="n">sum_exp_scores</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
            <span class="n">grad</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">percent_exp_score</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">sample_x</span>


        <span class="n">grad</span><span class="p">[</span><span class="n">correct_class</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-=</span> <span class="n">sample_x</span> <span class="c"># deal with the correct class</span>

    <span class="n">loss</span> <span class="o">/=</span> <span class="n">num_train</span>
    <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span> <span class="c"># add regularization</span>
    <span class="n">grad</span> <span class="o">/=</span> <span class="n">num_train</span>
    <span class="n">grad</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span>

<span class="k">def</span> <span class="nf">loss_grad_softmax_vectorized</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
    <span class="s">""" Compute the loss and gradients using softmax with vectorized version"""</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span> 
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
    <span class="n">dim</span><span class="p">,</span> <span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c"># [K, N]</span>
    <span class="c"># Shift scores so that the highest value is 0</span>
    <span class="n">scores</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="n">scores_exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="n">correct_scores_exp</span> <span class="o">=</span> <span class="n">scores_exp</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_train</span><span class="p">)]</span> <span class="c"># [N, ]</span>
    <span class="n">scores_exp_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">scores_exp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c"># [N, ]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">correct_scores_exp</span> <span class="o">/</span> <span class="n">scores_exp_sum</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="n">num_train</span>
    <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>

    <span class="n">scores_exp_normalized</span> <span class="o">=</span> <span class="n">scores_exp</span> <span class="o">/</span> <span class="n">scores_exp_sum</span>
    <span class="c"># deal with the correct class</span>
    <span class="n">scores_exp_normalized</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_train</span><span class="p">)]</span> <span class="o">-=</span> <span class="mi">1</span> <span class="c"># [K, N]</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">scores_exp_normalized</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">/=</span> <span class="n">num_train</span>
    <span class="n">grad</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span></code></pre></figure>

<h2 id="summary">10. Summary</h2>
<ul>
  <li>Logitic and softmax regression are similar and used to solve binary and multiple classification problems respectively. However, we can also use the logistic regression classifier to solve multi-classification based on one-vs-all trick.</li>
  <li>We should keep it in mind that logistic and softmax regression is based on the assumption that we can use a linear model to (roughly) distinguish different classes. So we should be very careful if we don’t known the distribution of the data.</li>
  <li>We use linear function to map the input X (such as image) to label scores y for each class: <script type="math/tex">scores = f(x^{(i)}, W, b) = Wx{(i)} + b</script>. And then use the largest score for prediction.</li>
  <li>Normalizing the scores from 0 to 1. Im my opinion here is the most fundamental idea of the losgistic and softmax regression (function): that is we use a non-linear (exponential function) instead of linear function for normalization. It is reasonable to interprete that the bigger the score of one class is, the even more chance the sample belongs to that category, and the it is better to make derivative strictly increasing (exponential function is an appropriate condidate). Then we normalized the scores by computing the perentage of exponent score of each class in total exponent scores for all classes.</li>
  <li>As for loss function, the idea is to make the loss small when the normalized score is large, and penlize more when normalized score is small. it is not hard to figure out to using <script type="math/tex">-log(x)</script> function because we use exponential function to preprocess the scores.</li>
  <li>After defining the loss function, we can use the gradient descent algorithm to train the model.</li>
  <li>For implementation, it is critical to use matrix calculation, however it is not straightforward to transfer the naive loop version to vectorized version, which requires a very deep understanding of matrix multiplication. I’ve implemented the two algorithms to solve the CIFAR-10 dataset, and for test datasets I’ve got 82.95% accuracy for binary classification, 33.46% for all 10-classification using one-vs-all concept and 38.32% for all 10-classification using Softmax regression.</li>
</ul>

<h2 id="reference-and-further-reading">11. Reference and further reading</h2>
<ul>
  <li>Andrew Ng’s <a href="https://www.coursera.org/course/ml">Machine learning on Coursera</a></li>
  <li>Machine learing notes on <a href="http://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf">Stanford Engineering Everywhere (SEE)</a></li>
  <li>Stanford University open course <a href="http://vision.stanford.edu/teaching/cs231n/">CS231n</a></li>
  <li>The University of Nottingham <a href="http://modulecatalogue.nottingham.ac.uk/Nottingham/asp/moduledetails.asp?year_id=000113&amp;crs_id=021211">Machine Learning Module</a></li>
</ul>

