<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Interpretation of matrix</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="From Coal Mining to Data Mining">
    <link rel="canonical" href="http://houxianxu.github.io/2015/05/18/matrix-multiply-interpretation/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Xianxu Hou's blog posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-61586501-1', 'auto');
      ga('send', 'pageview');
    </script>

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <div style="float:left; margin-top:10px; margin-right:10px;">
      <img src="/assets/avatar_Xianxu Hou.png" width="40">
    </a>
    </div>

    <a class="site-title" href="/">Xianxu Hou's blog</a>
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
          
        
          
        
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Interpretation of matrix</h1>
    <p class="meta">May 18, 2015</p>
  </header>

  <article class="post-content">
  <p>When I study and implement machine learning algorithm, it is crucial and tricky to use matrix-matrix multiplication (which generalizes all matrix-vector and vector-vector) to speed up algorithms. However it is difficult to interpret the vectorized expressions, which needs strong linear algebra background. This post summarizes some basic concept in linear algebra and focuses more on the interpretation, which could be very helpful for us to understand some machine learning algorithms such as Neural Networks (just a chain of matrix-matrix multiplication).</p>

<h2 id="n-linear-equation-with-n-unknowns">1. N linear equation with n unknowns</h2>

<p>Example with N = 2 (2-dimension):
<script type="math/tex">% <![CDATA[
\begin{cases}2x - y = 0\\-x + 2y = 3\end{cases} = \begin{bmatrix}2 & -1 \\-1 & 2 \end{bmatrix} \begin{bmatrix}x \\y \end{bmatrix} = \begin{bmatrix}0 \\3 \end{bmatrix} %]]></script></p>

<h4 id="row-picture">Row picture</h4>
<p>This is the way we often interpret the two equations above: just <strong>two line in 2-D space</strong>, and the solution is the point lies on both lines.</p>

<!-- ![Plot of Two equations](/images/linearAlgbra/1.png "Row picture") -->
<center><img src="/images/linearAlgbra/1.png" width="80%" /></center>

<h4 id="column-picture---linear-combination-of-columns">Column picture - linear combination of columns</h4>
<p>Follow the column we can rewrite the equations above as follows:</p>

<script type="math/tex; mode=display">x \begin{bmatrix}2 \\-1 \end{bmatrix}  + y \begin{bmatrix}-1 \\ 2 \end{bmatrix} = \begin{bmatrix}0 \\ 3 \end{bmatrix}</script>

<p>We can interpret the above equation as <strong>linear combination of columns</strong> which are vectors in 2-D, and the <strong>+</strong> is overloaded for 2-D vector addition, as compared with scalar addition in row picture interpretation. The geometry is shown below.</p>

<!-- ![Plot of Two equations](/images/linearAlgbra/2.png "column picture") -->
<center><img src="/images/linearAlgbra/2.png" width="80%" /></center>

<p>When considering high dimension problem (say n = 10, i.e., 10 linear equation with n unknowns), it is not easy to imagine n-D space from Row Picture. However from Column Picture, the result is just the linear combination of 10 vectors.</p>

<h2 id="matrix-multiplication-as-linear-combination">2. Matrix multiplication as linear combination</h2>
<p>Usually we do matrix multiplication is to get the result cell as the dot product of a row in the first matrix with a column in the second matrix. However there is a very good interpretation from linear combination aspect, which is a core concept in linear algebra.</p>

<h4 id="linear-combination-of-columns-of-matrix">2.1 Linear combination of columns of matrix</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}1 & 2 & 3\\ 4&5&6\\7&8&9 \end{bmatrix} \begin{bmatrix}a \\b \\c\end{bmatrix} 
= a \begin{bmatrix}1 \\ 4\\7 \end{bmatrix} + b \begin{bmatrix}2 \\ 5\\8 \end{bmatrix} + c \begin{bmatrix}3 \\ 6\\9 \end{bmatrix} = \begin{bmatrix}1a + 2b + 3c \\ 4a + 5b+6c\\7a+8b+9c \end{bmatrix} %]]></script>

<p>Representing the columns of matrix by colorful boxes will help visualize this as follows: (the picture is from <a href="http://eli.thegreenplace.net/2015/visualizing-matrix-multiplication-as-a-linear-combination/">Eli Bendersky</a>)</p>

<!-- ![matrix-vector](/images/linearAlgbra/3.png) -->
<center><img src="/images/linearAlgbra/3.png" width="80%" /></center>

<p>For matrix multiply a column vector, the result is a column vector which is the linear combination of the columns of the matrix and the coefficients are the second vector. This idea can also be generalized to Matrix-Matrix multiplication, i.e., the columns of the result matrix is the first matrix multiply each column (vector) in the second matrix respectively. The following picture shows the idea.</p>

<!-- ![matrix-matrix](/images/linearAlgbra/4.png "matrix-matrix") -->
<center><img src="/images/linearAlgbra/4.png" width="80%" /></center>

<h4 id="linear-combination-of-rows-of-matrix">2.2 Linear combination of rows of matrix</h4>
<p>Similarly we can view the matrix as different rows. 
<script type="math/tex">% <![CDATA[
\begin{bmatrix}a &b &c\end{bmatrix}  \begin{bmatrix}1 & 2 & 3\\ 4&5&6\\7&8&9 \end{bmatrix} 
= a \begin{bmatrix}1 & 2 & 3 \end{bmatrix} + b \begin{bmatrix}4&5&6 \end{bmatrix} + c \begin{bmatrix}7&8&9 \end{bmatrix} %]]></script>
The above equation can be represented as follows:</p>

<!-- ![vector-matrix](/images/linearAlgbra/5.png "vector-matrix") -->
<center><img src="/images/linearAlgbra/5.png" width="80%" /></center>

<p>For matrix-matrix multiplication, the rows of the result matrix is each row (vector) in first matrix multiply the second matrix. The idea can be represented graphically following:</p>

<!-- ![matrix-matrix](/images/linearAlgbra/6.png "matrix-matrix") -->
<center><img src="/images/linearAlgbra/6.png" width="80%" /></center>

<h4 id="column-row-multiplication">2.3 Column-row multiplication</h4>

<p>There is another interpretation of matrix multiplication from <script type="math/tex">column * row</script> view.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}a \\ b \\ c \end{bmatrix} \begin{bmatrix}x & y & z \end{bmatrix} = \begin{bmatrix}ax&ay&az \\ bx&by&bz \\ cx&cy&cz \end{bmatrix} %]]></script>

<p>The above result is a 3 by 3 matrix. And if we two matrix are m by n and n by p, the shape of the result matrix is m by p and the result is the sum of all the matrix (m by p) computed by all the <script type="math/tex">n^{th}</script> column in the first matrix and <script type="math/tex">n^{th}</script> row in the second matrix.</p>

<h4 id="block-matrix-multiplication">2.4 Block matrix multiplication</h4>
<p>There is even another amazing interpretation of matrix multiplication. It is often convenient to partition a matrix <strong>A</strong> into smaller matrices called blocks. Then we can treat the blocks as matrix entries when do matrix multiplication.</p>

<script type="math/tex; mode=display">% <![CDATA[
AB = \left[
\begin{array}{cc} a_{11} & a_{12} \\ a_{21} & a_{22} \end{array} \right]\cdot
\left[
\begin{array}{cc} b_{11} & b_{12} \\ b_{21} & b_{22} \end{array} \right]
= 
\left[
\begin{array}{cc} a_{11}b_{11}+a_{12}b_{21} & a_{11}b_{12}+a_{12}b_{22} \\ a_{21}b_{11}+a_{22}b_{21} & a_{22}b_{12}+a_{22}b_{22} \end{array} \right] %]]></script>

<p>which is equal to:</p>

<script type="math/tex; mode=display">% <![CDATA[
AB = \left[
\begin{array}{c|c} A_{11} & A_{12} \\\hline A_{21} & A_{22} \end{array} \right]\cdot
\left[
\begin{array}{c|c} B_{11} & B_{12} \\\hline B_{21} & B_{22} \end{array} \right]
= 
\left[
\begin{array}{c|c} A_{11}B_{11}+A_{12}B_{21} & A_{11}B_{12}+A_{12}B_{22} \\\hline A_{21}B_{11}+A_{22}B_{21} & A_{22}B_{12}+A_{22}B_{22} \end{array} \right] %]]></script>

<h4 id="elimination-matrices">2.5 Elimination matrices</h4>

<p>The linear combination interpretation of matrix multiplication is very useful for us to understand matrix transformation. Especially when we do row operation, we can achieve elimination to solve a system of linear equations. Take the following matrix multiplication for example AX=B, we want to choose the first matrix <strong>A</strong> in order to transform matrix <strong>X</strong> to <strong>B</strong>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}a &b &c \\ d&e&f \\ g&h&i \end{bmatrix} \begin{bmatrix}1 & 2 & 3\\ 4&5&6\\7&8&9 \end{bmatrix} = \begin{bmatrix}1 & 2 & 3\\ 0&-3&-6\\7&8&9 \end{bmatrix} %]]></script>

<p>Recall that the row in result matrix the row in <strong>A</strong> multiply <strong>X</strong>, which is the linear combination of rows of <strong>X</strong>. Because the first and third row is the same, so the first and third row in A should be <script type="math/tex">[1 \: 0 \: 0]</script> and <script type="math/tex">[0 \: 0 \: 1]</script> and the second row in <strong>B</strong> is the second row minus 4 times first row in <strong>A</strong>, i.e., <script type="math/tex">[row2 - 4*row1]</script>. So the second row in <strong>A</strong> should be <script type="math/tex">[-4 \: 1 \: 0]</script>. Put all together A = <script type="math/tex">% <![CDATA[
\begin{bmatrix}1 & 0 & 0\\ -4&1&0\\0&0&1 \end{bmatrix} %]]></script></p>

<h4 id="permutation-of-matrix">2.6 Permutation of matrix</h4>
<p>Exchange the two rows in a matrix <strong>A</strong>, we just need to multiply some matrix on the left as shown as follows. For example in the result matrix, the first row is the linear combination of the rows in the second matrix with respect to first row in the first matrix. What we want is the second row, so the second cell in the first matrix should be 1, and first cell should be 0, which has no contribution to the first row in the result matrix.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix}a & b \\ c & d \end{bmatrix} = \begin{bmatrix}c & d \\ a & b \end{bmatrix} %]]></script>

<p>If want to exchange the columns, we just need to do column operation:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}a & b \\ c & d \end{bmatrix} \begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}  = \begin{bmatrix}b & a \\ d & c \end{bmatrix} %]]></script>

<p><strong>In short, if we want to do column operations the matrix multiplies on the right, and to do row operations, it multiplies on the left.</strong></p>

<p>##3 Inverse or non-singular matrix
Suppose <strong>A</strong> is square matrix, and if <script type="math/tex">A A^{-1} = I</script> (<script type="math/tex">I</script> is identity matrix), then matrix A is invertible and the inverse matrix is <script type="math/tex">A^{-1}</script>. We can see whether inverse matrix is a property for a given matrix, and not all matrices have inverse matrix. One simple way to determine whether you can find a vector <strong>x</strong> != <strong>0</strong> with <strong>Ax = b</strong>, and if you cannot find a <strong>x</strong>, then A has inverse matrix, otherwise not. For example,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}1 & 3 \\ 2 & 6 \end{bmatrix} \begin{bmatrix}3  \\ -1 \end{bmatrix} = \begin{bmatrix}0\\0 \end{bmatrix} %]]></script>

<p>There couldn’t be an inverse for the first matrix above. We can think that the first and second column (vector) are in same direction, the linear combination of them can not be <strong>0</strong>.</p>

<p>If matrix A does has inverse matrix, then Guass-Jordan elimination can solve it: <script type="math/tex">E[A I] = [I A^{-1}]</script>, where <script type="math/tex">EA = I</script>. You interpret the equation as block matrix multiplication. We can use inverse matrix to factorize matrix as two matrix multiplication. First use elimination (or elemental) matrix <strong>E</strong> to transform <strong>A</strong> into <strong>U</strong>, i.e. <script type="math/tex">EA = U</script>, then solve A as <script type="math/tex">A = E^{-1}U = LU</script>. The factors <strong>L</strong> and <strong>U</strong> are triangular matrices. Because of <strong>U</strong> is the result of elimination, so it should be a upper triangular matrix. For <strong>L</strong>, we can use Guass-Jordan elimination <script type="math/tex">E[A I] = [I A^{-1}]</script> to compute the <strong>L</strong>, because the we just do elimination of <strong>I</strong> with <strong>E</strong>, so the cell values in the upper bound are all zero, so <strong>L</strong> is a lower triangular matrix. If we need to exchange rows, all we need is to multiply a permutation matrix on the left: <script type="math/tex">PA = E^{-1}U = LU</script>.</p>

<p>Another property of invertible square matrix is that you can exchange transposing and inversing for a singular matrix. <script type="math/tex">(A^{-1})^T \: A^T = I</script>, i.e., <script type="math/tex">(A^{-1})^T = (A^T)^{-1}</script>. More more interesting thing is that when a matrix multiply its transpose, we get a symmetry matrix. <script type="math/tex">(A A^{T})^T = (A^T)^T A^T = AA^T</script>.</p>

<h2 id="vector-spaces">3. Vector Spaces</h2>
<p>Vector space means the “space” of vectors, which should satisfy some rules, i.e., we can multiply any vector v by any scalar c in that space, that’s to say they can produce <strong>linear combination</strong>. For example, <script type="math/tex">\mathbb{R}^2</script> space contains all the real vectors with 2 components and it represents x-y plane, and <script type="math/tex">\mathbb{R}^2</script> space contains all the real vectors with 2 components and it represents x-y plane, and <script type="math/tex">\mathbb{R}^3</script> space contains all the real vectors with 3 components and it represents x-y-z 3-d space.</p>

<p>Subspace is a vector space which contains some or all the vectors from another vector space. Subspace should be satisfy the definition of space (linear combination) and it is based on another vector space. For instance, there are 4 subspace of <script type="math/tex">\mathbb{R}^3</script>: <strong>Z</strong> – the single vector (0 0, 0); (<strong>L</strong>) – any line through (0, 0, 0); (<strong>P</strong>) – any plane through(0, 0, 0); <script type="math/tex">\mathbb{R}^3</script> – the whole space.</p>

<p>In return, we can use some vectors or a matrix to construct vector space because all we need is linear combination. Given a matrix, the linear combination of all the columns of matrix from a space, which is called <strong>column space</strong>. For example:</p>

<script type="math/tex; mode=display">% <![CDATA[
A = \begin{bmatrix}1 & 2 \\ 3 & 4 \\ 5&6 \end{bmatrix} %]]></script>

<p>The column vector of A is in <script type="math/tex">mathbb{R}^3</script>, and all the combinations of columns form a subspace, which is plane through origin. And the intersection of two subspace is a subspace.</p>

<p>##4. Interpret <script type="math/tex">Ax=b</script> with vector space
 #### column space
We can interpret <script type="math/tex">Ax</script> (<strong>A</strong> is a matrix and <strong>x</strong> is a vector) as linear combination of columns of matrix using vector x, i.e., which is the columns space define by matrix A. So we can view <script type="math/tex">Ax=b</script> as finding the perfect linear combination of the columns to make it equal to vector <strong>b</strong>, and the vector <strong>b</strong> should be in the column space defined by matrix <strong>A</strong>.</p>

<script type="math/tex; mode=display">% <![CDATA[
Ax = \begin{bmatrix}1 & 5 &6\\ 2 & 6 & 8\\ 3&7&10 \\4&8&12 \end{bmatrix} \begin{bmatrix}x \\ y \\ z \end{bmatrix} = b %]]></script>

<p>Take above equation for example, not for all b we can find a solution. Because <strong>b</strong> could be any vector in <script type="math/tex">\mathbb{R}^4</script> and the left hand side the combinations of 3 columns don’t fill the whole 4-D space. The fact is that there are a lot of vectors b are not the combinations of the 3 columns (column subspace which is inside <script type="math/tex">\\mathbb{R}^4</script>). We can only solve the equation when <strong>b</strong> is in the column space of <strong>A</strong>, for example, <script type="math/tex">x = [1 \: 0 \: 0]^T</script>, when <script type="math/tex">b = [1 \: 2 \: 3 \: 4]^T</script>.</p>

<h4 id="null-space">Null space</h4>
<p>Particularly for equation <script type="math/tex">Ax=b=0</script>, we can get a bunch of vectors <strong>x</strong> as the solutions and this vectors can compose a subspace because <script type="math/tex">A(v + w) = Av + Aw = 0</script>. Take example above (when b = 0), the solution is <script type="math/tex">c[1\:1\:-1]^T</script> (c is constant), which is called <strong>Null space</strong>. And if <script type="math/tex">b != 0</script>, then the solution is not a subspace because <strong>0</strong> is not in that bunch of vectors.</p>

<h2 id="compute-ax--b">5. Compute <script type="math/tex">Ax = b</script></h2>
<h4 id="null-space-b--0">Null space b = 0</h4>
<p>We can do elimination for matrix A and get the matrix <strong>U</strong>, and then continue simplifying the <strong>U</strong> to get matrix <strong>R</strong> which is reduced row echelon form and <strong>R</strong> has the form of</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}I&F\\ O&O\\\end{bmatrix} %]]></script>

<p><script type="math/tex">I</script> is identity matrix and indicates pivot variables. In fact the particular solutions are the columns of matrix
<script type="math/tex">% <![CDATA[
N = {\begin{bmatrix}-F &I\end{bmatrix}}^T %]]></script>, the null space is the column space of N. This is because of following matrix block multiplication.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}I&F\\ O&O\\\end{bmatrix} {\begin{bmatrix}-F \\ I\end{bmatrix}} = O %]]></script>

<h4 id="b--0">b != 0</h4>
<p>First we should consider whether the equation has solution or not, as mentioned above, <strong>Ax = b</strong> is solvable when b is column space of A, i.e., C(A). On the other hand, after finishing the elimination step, if the a combination of rows of A gives zero rows, the same combination of entries of b must give 0.</p>

<p>If the equation does have solutions, we can use elimination to find a particular solution. As long as we get one particular solution, the complete solution is the particular solution plus the any vector in the null space of <strong>A</strong>. that’s to say, <script type="math/tex">x = x_{particular} + x_{null}</script>. The shape of the complete solution is similar to Null space, we can interpret the complete space as null space which is shifted by vector <script type="math/tex">x_{particular}</script>. This is because:</p>

<script type="math/tex; mode=display">Ax_p = b \: \: and \:\:Ax_n = 0 \:\:= \: A(x_p + x_b) = b</script>

<h4 id="solution-discussion----m-by-n-matrix-a-of-rank-r">Solution discussion – m by n matrix A of rank r</h4>
<p><strong>Full column rank, i.e., r = n</strong></p>

<ol>
  <li>There are free variables</li>
  <li>The null space is {<strong>0</strong>}</li>
  <li>Unique solution if it exists (0 or 1 solution)</li>
  <li>The reduced row echelon form is <script type="math/tex">\begin{bmatrix}I\\ O\\\end{bmatrix}</script>.</li>
</ol>

<p><strong>Full row rank, i.e., r = m</strong></p>

<ol>
  <li>It can be solved Ax=b for every b, because every row have a pivot and no zero rows.</li>
  <li>There are n - r free variables and there are infinite solutions.</li>
  <li>The reduced row echelon form is <script type="math/tex">% <![CDATA[
\begin{bmatrix}I&F\end{bmatrix} %]]></script></li>
</ol>

<p><strong>Full column and row rank, i.e., r = m = n</strong></p>

<ol>
  <li>Invertible matrix of A</li>
  <li>Unique solution</li>
  <li>The reduced row echelon form is identity <strong>I</strong>.</li>
</ol>

<p><strong>Not full rank, i.e., r &lt; m, and r &lt; n</strong></p>

<ol>
  <li>There are no solutions or infinite solutions</li>
  <li>The reduced row echelon form is <script type="math/tex">% <![CDATA[
\begin{bmatrix}I&F\\0&0\end{bmatrix} %]]></script></li>
</ol>

<h2 id="independent-span-basis-dimension">6. Independent, span, basis dimension</h2>
<p>Independent is used to describe the relation between vectors. Vectors <script type="math/tex">v_1, v_2, ..., v_n</script> are Independent if no linear combination gives zero vector (except the zero combination), i.e., <script type="math/tex">c_1 v_1 + c_2 v_2 + ... + c_n v_n != 0</script>. From vector space point of view, <script type="math/tex">v_1, v_2, ..., v_n</script> are columns of matrix <strong>A</strong>, they are independent if null space of A is zero vector and the rank r = n with no free variables, and they are independent if Ac = 0 for some non-zero vector c and rank &lt; n with free variables.</p>

<p>Vectors <script type="math/tex">v_1, v_2, ..., v_n</script> <strong>span</strong> a vector space means that the space contains all the linear combination of those vectors. They vectors could be independent or dependent.</p>

<p>We are more interested in the vectors spanning a space are independent, which means the right number or minimal number of vectors to span a given space and we use <strong>basis</strong> to indicate this idea. Basis for a vector space is segment of vectors with 2 properties: (1) They are independent; (2)They span a space.</p>

<p>For a given space such as <script type="math/tex">\mathbb{R}^4</script>, every basis has the same number of vectors and the number is called <strong>dimension of the space</strong>. So when putting all together, we get the conclusion the rank of a matrix <strong>A</strong> == the number of pivot columns == dimension of the column space.</p>

<h2 id="orthogonal">7. Orthogonal</h2>
<p>Vector x is orthogonal to vector y, when <script type="math/tex">x^T y = 0</script>
Subspace S is orthogonal to subspace T means: every vector in S is orthogonal to every vector in T. For every space, the row space is orthogonal to nullspace. Because of Ax = 0, so the linear combination of rows respecting to null space is 0, i.e. <script type="math/tex">\sum_i^m c_i \: row_i = 0</script>
Moreover, nullspace and row space are orthogonal complements in <script type="math/tex">R^n</script> and nullspace contains all vectors perpendicular to the row space.</p>

<h2 id="solve-ax--b-when-there-are-no-solutions">8. “Solve” <script type="math/tex">Ax = b</script> when there are no solutions</h2>
<p>We know that <script type="math/tex">Ax = b</script> is only solvable when vector <strong>b</strong> is in the column space of A. In practice, this equation is often unsolvable when A is a rectangular. Take m by n matrix (m  n)for example, there are more constrains or equations than unknown variables and there may be no solutions when some equations conflict each other. In other words, there is a lot of information about x here. One naive method is only using some of information (equations), however, there is no reason to say some equations are perfect and some are useless and we want to use all the information to get the best solution.</p>

<p>When <script type="math/tex">Ax = b</script> cannot be solved perfectly, what can we do? And can we do better? The reason that the equation is not solvable is because <strong>b</strong> is not in the column space of <strong>A</strong>, so we may be able to find a “closest” vector (say <strong>p</strong>) to replace <strong>b</strong> in the column space of <strong>A</strong>, i.e., <script type="math/tex">A\hat{x} = p</script>, and use <script type="math/tex">\hat{x}</script> to estimate <script type="math/tex">x</script>. The next problem is to define the “best” <strong>p</strong>, and the projection of <strong>b</strong> onto column space <strong>A</strong> is used instead. And then we need to find a way to calculate the projected vector <strong>p</strong> from <strong>b</strong>.</p>

<!-- ![projection from vector to vector](/images/linearAlgbra/7.png) -->
<p><img src="/images/linearAlgbra/7.png" width="100%" /></p>

<p>For simplicity, we first consider projection from vector to vector (see the left diagram above). We use <strong>p</strong> to indicate the projection of <strong>b</strong> onto <strong>a</strong> and <strong>b</strong> is equal to some multiple of <strong>a</strong>, i.e., <script type="math/tex">xa</script>. According to is perpendicular to e, we have:</p>

<script type="math/tex; mode=display">a^T e = a^T (b-xa) = a^Tb - xa^Ta = 0 \: = \: x = \frac{a^Tb}{a^Ta} \\
So \: p = xa = \frac{a^Tb}{a^Ta} a \\
We \: can \: rewrite \: as \: p = P b = \big(\frac{a a^T}{a^T a}\big) b</script>

<p>Notice that <strong>P</strong> is a n by n matrix if vector b and a have n elements and it is determined only by the vector <strong>a</strong> which we want to project onto. P is called projection matrix and we can interpret it as a <strong>function</strong> coming from vector <strong>a</strong> to project another vector to itself. Additionally we can observe that <script type="math/tex">P^T = (\frac{a a^T}{a^T a})^T = P</script> and <script type="math/tex">PP = \frac{a a^T}{a^T a} \frac{a a^T}{a^T a} = \frac{a (a^T a) a^T} {(a^T a) (a^T a)} = P</script></p>

<p>Next we consider projection from vector to space (see the right diagram above), the plane is the column space of A = [a1 a2], the error <strong>e = b - p</strong> and it is perpendicular to <strong>A</strong>. The projection <strong>b</strong> to <strong>A</strong> is <script type="math/tex">a_1 x_1 + x_2 x_2 = A x</script> and our aim is to find <strong>x</strong>. Because <strong>e = b - Ax</strong> is perpendicular to the plane:</p>

<script type="math/tex; mode=display">\begin{cases}a_1^T(b-Ax) = 0\\a_2^T(b-Ax) = 0\end{cases} \: = \begin{bmatrix}a_1^T \\a_2^T\end{bmatrix} \begin{bmatrix}b-Ax\end{bmatrix} = \begin{bmatrix}0 \\0\end{bmatrix} = A^T \begin{bmatrix}b-Ax\end{bmatrix} = 0 \\ = A^Tb - A^TA x = 0 \: = x = (A^TA)^{-1} A^T b \\ = p = A x = A (A^TA)^{-1} A^T b = P b</script>

<p>From above equation we can interpret the (b-Ax) is in the null space of <script type="math/tex">A^T</script>, and the (b-Ax) should be perpendicular to the row space of <script type="math/tex">A^T</script> which is the column space of A, which is the plane defined by <strong>a</strong> and <strong>b</strong>.
The matrix <script type="math/tex">P = A(A^TA){-1} A^T</script> is called projection matrix, which can also be interpreted as a function to project a vector b onto the column space of <strong>A</strong>. Moreover <script type="math/tex">P^T = P</script> and <script type="math/tex">PP = P</script>. Here if A has independent columns then <script type="math/tex">A^TA</script> is invertible. Here is a simple proof: suppose <script type="math/tex">A^TAx=0</script>, then x must be 0 if <script type="math/tex">A^TA</script> is invertible.</p>

<script type="math/tex; mode=display">A^TAx=0 \:= x^TA^TAx =0\:=(Ax)^T(Ax) = 0\:=Ax = 0=x=0</script>

<h2 id="determinant">9. Determinant</h2>
<p>The square matrix is relatively easy to deal with, and the determinant is a number that associates with a square matrix, <script type="math/tex">det\: A=\|A\|</script>. Though this number can’t not tell you all the information of the matrix, it can tell you a lot of information. Following is some important properties of determinant.</p>

<ul>
  <li>Determinant of <strong>I</strong> is 1</li>
  <li>Exchange two rows of a matrix: reverse sign of determinant.</li>
  <li>Linear combination of one row, and det (A+B) != det(A) + det(B)</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{vmatrix}ta &tb \\c &d \end{vmatrix} = t\begin{vmatrix}a &b \\c &d \end{vmatrix}\:\:\:\: and \:\:\: \: \begin{vmatrix}a + a'&b'+b \\c &d \end{vmatrix} = \begin{vmatrix}a &b \\c &d \end{vmatrix} + \begin{vmatrix}a' &b' \\c &d \end{vmatrix} %]]></script>

<ul>
  <li>Two equal rows, then determinant is 0. We can get it by exchanging the same row.</li>
  <li>Subtract l * row i from row k, and the determinant doesn’t change, i.e. elimination process.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{vmatrix}a &b \\c-la &d-lb \end{vmatrix} = \begin{vmatrix}a &b \\c &d \end{vmatrix} -l \begin{vmatrix}a &b \\a &b \end{vmatrix} = \begin{vmatrix}a &b \\c-la &d-lb \end{vmatrix} %]]></script>

<ul>
  <li>Row of zeros, the determinant is equal to 0</li>
  <li>Triangular matrix, the determinant is product of pivots, which is based on property 3.</li>
  <li>det A = 0, when A is singular, and det A != 0 when A is invertible.</li>
  <li>det(AB) = det(A)*det(B), and <script type="math/tex">det(A^{-1}) = \frac{1}{det(A)}</script></li>
  <li>
    <script type="math/tex; mode=display">% <![CDATA[
det(A^T) = det(A) \:\: |A^T| = |A| <= |U^TL^T| = |LU| <= |U^T||L^T| = |L||U| %]]></script>
  </li>
</ul>

<p>Cofactor of one entry
Cofactor of <script type="math/tex">a_{ij} = c_ij</script> is the (+/-) determinant of n-1 matrix with row i and column j erased, when i+j is even choose +, and choose - when i+j is odd. 
Cofactor formula is: <script type="math/tex">det(A) = a_{i1}c_{i1} + a_{i2}c_{i2} + ... + a_{in}c_{in}</script></p>

<p>Another very good interpretation is that the determinant is the <strong>volume</strong> of a box determined by row vectors.</p>

<h2 id="eigenvalue-and-eigenvector">10. Eigenvalue and eigenvector</h2>
<p>The result vector <script type="math/tex">Ax</script> is parallel to x, and the vector <strong>x</strong> is called eigenvector of square matrix A, i.e.,</p>

<script type="math/tex; mode=display">Ax = \lambda x</script>

<p>The <script type="math/tex">\lambda</script> is called eigenvalue.</p>

<script type="math/tex; mode=display">Ax = \lambda x \:\: = \:\: (A - \lambda I) x = 0</script>

<p>Notice that <script type="math/tex">(A - \lambda I)</script> should be singular, otherwise x must be 0. So the det<script type="math/tex">(A - \lambda I)</script> = 0, and after finding the eigenvalues we can find the eigenvectors by computing null space. For most n by n matrix, there are n eigenvectors and eigenvalues. Some matrix just has one eigenvectors and eigenvalues of some matrix are complex number.</p>

<p>Above we know how to compute the eigenvalues and eigenvectors, then how to use them. Suppose there are n independent eigenvectors of n by n matrix A, and put them in column matrix S.</p>

<script type="math/tex; mode=display">% <![CDATA[
AS = A[x_1, x_2, ..., x_n] = [\lambda_1x_1, \lambda_2x_2, ..., \lambda_nx_n] = [x_1, x_2, ..., x_n] \begin{bmatrix}\lambda_1&0&\cdots&0\\
0&\lambda_2&\cdots&0\\
0&0&\ddots&0\\
0&0&\cdots&\lambda_n\end{bmatrix} = S\Lambda %]]></script>

<p><script type="math/tex">\Lambda</script> is called diagonal eigenvalue matrix</p>

<script type="math/tex; mode=display">AS = S\Lambda \:\: = \:\: S^{-1} AS = \Lambda \:\: =\:\: A = S\Lambda S^{-1}</script>

<p>In fact, the eigenvectors and eigenvectors give a way what is going on inside a matrix and to understand the power of matrix. For example, <script type="math/tex">A^k - 0 \:\:as\:\: k - \infty</script> if all <script type="math/tex">% <![CDATA[
\|\lambda_i\| < 1 %]]></script>.</p>

<script type="math/tex; mode=display">Ax = \lambda x \:\: =\:\: A^2 x = \lambda Ax = {\lambda}^2 x \\
A^2 = S \Lambda S^{-1} S \Lambda S^{-1} = S {\Lambda}^2 S^{-1} \\
A^k = S {\Lambda}^k S^{-1}</script>

<p>Base on the above equation, A is sure to have n independent eigenvectors and can be diagonalizable if all the eigenvalues <script type="math/tex">\lambda_i</script> are different.</p>

<p>We can use eigenvectors to solve following problem: <script type="math/tex">u_{k+1} = Au_k</script>, start with a give vector <script type="math/tex">u_0</script>, and we can see <script type="math/tex">u_k = A^k u_0</script>.
To really solve above equation, we can first write <script type="math/tex">u_0</script> as the linear combination of eigenvectors of matrix A. Then:</p>

<script type="math/tex; mode=display">u_0 = c_1 x_1 + c_2 x_2 + ... + c_n x_n\\
= A u_0 = A(c_1 x_1 + c_2 x_2 + ... + c_n x_n) = c_1 \lambda_1 x_1 + c_2 \lambda_2 x_2 + ... + c_n \lambda_n x_n\\
= A^2 u_0 = A A u_0 = A (c_1 \lambda_1 x_1 + c_2 \lambda_2 x_2 + ... + c_n \lambda_n x_n) = c_1 {\lambda_1}^2 x_1 + c_2 {\lambda_2}^2 x_2 + ... + c_n {\lambda_n}^2 x_n\\
=A^k u_0 = c_1 {\lambda_1}^k x_1 + c_2 {\lambda_2}^k x_2 + ... + c_n {\lambda_n}^k x_n = S \Lambda c</script>

<p>Using above idea to solve Fibonacci problem, [0, 1, 1, 2, 3, 5, 8, 13, …], then how to get <script type="math/tex">F_100</script>. So we need to find how fast the sequence grows, which lies in the eigenvalues.</p>

<p>We want the form <script type="math/tex">u_{k+1} = A u_k</script> to use the matrix A. And we can construct the matrix as follows</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{cases}F_{k+2} = F_{k+1} + F_k\\F_{k+1} = F_{k+1}\end{cases} \:= \: u_{k+1} = Au_k = \begin{bmatrix}1&1\\1&0\end{bmatrix}u_k \:\: and \:\: u_{k} = \begin{bmatrix}F_{k+1}\\F_k\end{bmatrix}\\\\
A = \begin{bmatrix}1&1\\1&0\end{bmatrix} \:=\: |A-\lambda I| = {\lambda}^2 - \lambda - 1 = \lambda_1 =  \frac{1+\sqrt{5}}{2} \approx 1.618 \:\: \lambda_2 = \frac{1-\sqrt{5}}{2}  \approx  -0.618 \\
Eigenvectors \: are \: x_1 = [\lambda_1\:1]^T, \: x_2 = [\lambda_2 \:1]^T
u_0 = [1\: 0]^T = c_1 x_1 + c_2 x_2 \:=\: c_1 = 0.447 \: c_2 = -0.447\\
u_{100} = A^{100} u_0 = c_1 {\lambda_1}^{100} x_1 + c_2 {\lambda_2}^{100} x_2 = 0.447 (1.618)^{100} [1.618\:1]^T - 0.477 (-0.618)^{100} [-0.618\:1] \\
u_{100} \approx 0.447 (1.618)^{100} [1.618\:1]^T %]]></script>

<h2 id="first-order-differential-equation-du--dt--au">11. First order differential equation <script type="math/tex">d_u / d_t = Au</script></h2>
<p>We also arrange two or more equation into matrix form and try to solve it from matrix aspect. For example:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{cases}\frac{du_1}{dt} = -u_1 + 2u_2\\\frac{du_2}{dt} = u_1-2u_2\end{cases} \:=\: \frac{du}{dt} = A\begin{bmatrix}u1\\u2\end{bmatrix} = \begin{bmatrix}-1 &2\\1&-2\end{bmatrix} \begin{bmatrix}u1\\u2\end{bmatrix} %]]></script>

<p>We can see from the above equation, u2 and u1 can be affected by each other and the relationship or all information, in fact, is the matrix A. So we maybe solve the differential equations as a single system. Usually this system is called Linear Dynamic System.</p>

<p>We can use eigenvectors and eigenvalues of matrix A to solve above system. The result is very simple <script type="math/tex">u(t) = e^{tA}u(0)</script>. The key point is the matrix exponential, which can be interpreted by Taylor Series.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
     \begin{split} 
e^{At} &= I + At + \frac{(At)^2}{2!} + \frac{(At)^3}{3!} + ... + \frac{(At)^n}{n!} + ... \\ 
&= SIS^{-1} + S \Lambda S^{-1} t + \frac{S {\Lambda}^2 S^{-1} t}{2!} + \frac{S {\Lambda}^n S^{-1} t}{n!} + ... \\
&= S(I + \Lambda t + \frac{ {\Lambda}^2 t}{2!} + \frac{ {\Lambda}^n t}{n!} + ...)S^{-1} \\
&= Se^{\Lambda t}S{-1}
	\end{split}
    \end{equation} %]]></script>

<p>In the above equation, the S and <script type="math/tex">\Lambda</script> are defined by eigenvectors and eigenvalues. Notice that the equation should based on the fact that there are n independent eigenvectors of matrix A, i.e., A can be diagonalized. Moreover the <script type="math/tex">e^{\Lambda t}</script> can be easily computed as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
     \begin{split}
e^{\Lambda t} 
&= I + \Lambda t + \frac{ {\Lambda}^2 t}{2!} + \frac{ {\Lambda}^n t}{n!} + ... \\
&= I + {\begin{bmatrix}\lambda_1&0&\cdots&0\\
0&\lambda_2&\cdots&0\\
0&0&\ddots&0\\
0&0&\cdots&\lambda_n\end{bmatrix}}^1 + ... + \frac{1}{n!}{\begin{bmatrix}\lambda_1&0&\cdots&0\\
0&\lambda_2&\cdots&0\\
0&0&\ddots&0\\
0&0&\cdots&\lambda_n\end{bmatrix}}^n + ...\\
&= \begin{bmatrix}e^{\lambda_1 t}&0&\cdots&0\\
0&e^{\lambda_2 t}&\cdots&0\\
0&0&\ddots&0\\
0&0&\cdots&e^{\lambda_n t}\end{bmatrix} \\
\end{split}
    \end{equation} %]]></script>

<h2 id="symmetric-matrix-and-positive-definite">12. Symmetric matrix and positive definite</h2>
<p>Symmetric matrix is very special matrix and they are <strong>good</strong> matrices: the eigenvalues are REAL and eigenvectors can be chosen PERPENDICULAR. For usual case <script type="math/tex">A = S \Lambda S^{-1}</script>, for symmetric matrix <script type="math/tex">A = Q \Lambda Q^{-1} = Q \Lambda Q^T</script>, and the matrix Q is the orthonormal eigenvectors matrix.</p>

<p>In additional, <script type="math/tex">A = Q \Lambda Q^T = \sum_i^n \lambda_i q_i {q_i}^T</script>, so every symmetric matrix is a combination of perpendicular projection matrices. This is because <script type="math/tex">q_i {q_i}^T = \frac{q_i {q_i}^T}{q_i^T q_i}</script>, which is projection matrices.</p>

<p>Then how about the sign of <script type="math/tex">\lambda_i</script>, and the eigenvalues decides between instability and stability in differential equations.
<strong>Positive definite</strong> means that all the eigenvalues are positive. Positive semidefinite is called when eigenvectors are greater or equal to zero. Formally the Positive definite of matrix A is defined as the quadratic form is greater that zero, i.e., <script type="math/tex">x^TAx  0</script></p>

<script type="math/tex; mode=display">% <![CDATA[
x^TAx = \begin{bmatrix}x &y\end{bmatrix} \begin{bmatrix}a &b \\ c &d\end{bmatrix} \begin{bmatrix}x \\y\end{bmatrix} = ax^2 + (b+c)xy + dy^2 %]]></script>

<p>Another fact is that the signs of pivots are the same as signs of <script type="math/tex">\lambda_i</script> and the number of positive pivot is equal to the number of positive eigenvalues.</p>

<p>We can interpret the quadratic form <script type="math/tex">x^TAx = 1</script> as tiled ellipse associated with symmetric matrix A.  <script type="math/tex">X^T \Lambda X = 1</script> is a lined-up ellipse associated with eigenvalues matrix <script type="math/tex">Lambda</script>, and we can use eigenvectors matrix Q to rotate the tiled ellipse to lined-up ellipse. Here is an example:</p>

<script type="math/tex; mode=display">% <![CDATA[
5x^2 + 8xy + 5y^2 = 1 \: =\: \begin{bmatrix}x& y\end{bmatrix}A \begin{bmatrix}x\\y\end{bmatrix} = \begin{bmatrix}x& y\end{bmatrix} \begin{bmatrix}5 & 4\\4 & 5\end{bmatrix} \begin{bmatrix}x\\y\end{bmatrix} %]]></script>

<p>Use eigenvectors and eigenvalues to diagonalized <script type="math/tex">A = Q \Lambda Q^T</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}5 & 4\\4 & 5\end{bmatrix} = \frac{1}{\sqrt{2}} \begin{bmatrix}1 & 1\\ 1& -1\end{bmatrix} \begin{bmatrix}9 & 0\\0 & 1\end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix}1 & 1\\1 & -1\end{bmatrix} %]]></script>

<p>So <script type="math/tex">5x^2 + 8xy + 5y^2 =9(\frac{x+y}{\sqrt{2}})^2 + 1(\frac{x-y}{\sqrt{2}})^2 = 9X^2 + 1(Y)^2 = 1</script>. We can see that the axes of the titled ellipse point are along the eigenvectors and the axis lengths are determined by eigenvalues, i.e. <script type="math/tex">\frac{1}{\sqrt{\lambda_i}}</script></p>

<p>So in the xy system, the axes are along he eigenvectors of A. In the XY system, the axes are along the eigenvectors of <script type="math/tex">\Lambda</script>. So if <script type="math/tex">A=Q \Lambda Q^T</script> is positive definite, i.e., <script type="math/tex">\Lambda_i  0</script>. The graph <script type="math/tex">x^TAx =1</script> is an ellipse:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}x& y\end{bmatrix} Q \Lambda Q^T \begin{bmatrix}x\\y\end{bmatrix} = \begin{bmatrix}X& Y\end{bmatrix} \Lambda \begin{bmatrix}X\\Y\end{bmatrix} = \lambda_1 X^2 + \lambda_2 Y^2 = 1. %]]></script>

<p><script type="math/tex">A=Q \Lambda Q^T</script> is called the <strong>principal axis theorem</strong>, which can help displays the axes, or rotate to a lined-up position.</p>

<h2 id="singular-value-decomposition-svd">Singular value decomposition (SVD)</h2>
<p>For a full rank square matrix, we can diagonalize the matrix as <script type="math/tex">S^{-1}AS</script>. However, if A is any m by n matrix with rank r, can we still diagonalize it?</p>

<p>Based on the symmetric matrix, suppose A can be diagonalized as <script type="math/tex">A = U \Sigma V^T</script>, U and V are orthonormal matrix, <script type="math/tex">\Sigma</script> is a diagonal matrix. The idea in fact is very simple, that’s we make A to be symmetric matrix.</p>

<script type="math/tex; mode=display">A^TA = V \Sigma^T U^T U \Sigma V^T = V \Sigma^2 V^T = Q_1 \Lambda Q_1^T\\
AA^T = U \Sigma V^T V \Sigma^T U^T = U \Sigma^2 U^T = Q_2 \Lambda Q_2^T</script>

<p>Because <script type="math/tex">A^TA</script> and <script type="math/tex">AA^T</script> are symmetric matrix, we easily get V, U and <script type="math/tex">\Sigma</script>. In details, <script type="math/tex">v_1, ... v_r</script> are orthonormal singular vectors in row space of A, the <script type="math/tex">u_1, ..., u_r</script> are orthonormal simple vectors in column space. We need n-r more v’s and m-r u’s from the null space N(A) and left null space <script type="math/tex">N(A^T)</script>, <script type="math/tex">Vv_n=0v_m</script>, and we set <script type="math/tex">\sigma_i =0</script> when i  r.</p>

<script type="math/tex; mode=display">% <![CDATA[
A[v_1, ..., v_r, ..., v_n] = [u_1, ... ,u_r, ..., u_m] \begin{bmatrix}\sigma_1&0&\cdots&0\\
0&\sigma_r&\cdots&0\\
0&0&\ddots&0\\
0&0&\cdots&0\end{bmatrix} %]]></script>

<h2 id="reference-and-further-reading">9. Reference and further reading</h2>
<ul>
  <li><a href="http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/">Introduction to Linear Algebra from MIT OpenCourseWare</a></li>
</ul>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  
  <!-- disqus comments -->
 
 <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'houxianxu'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  
  
</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">Xianxu Hou's blog</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>Xianxu Hou's blog</li>
        <!-- <li><a href="mailto:"></a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/houxianxu">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">houxianxu</span>
          </a>
        </li>
        
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">From Coal Mining to Data Mining</p>
    </div>

  </div>

</footer>


    </body>
</html>